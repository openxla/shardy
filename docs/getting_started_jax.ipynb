{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ABTeSA_QZh"
      },
      "source": [
        "# Shardy Guide for JAX Users\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/openxla/shardy/blob/main/docs/getting_started_jax.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8HlD6EELrYt"
      },
      "source": [
        "Shardy is a new propagation system being introduced into the XLA stack, and below we want to introduce any JAX users to:\n",
        "\n",
        "1. What has changed in JAX\n",
        "2. Why Shardy?\n",
        "3. Future plans\n",
        "\n",
        "This is meant for JAX users who use `jax.jit` for running training/inference models across more than 1 GPU or TPU (batch parallelism, megatron, ZeRO, etc). They would be using things like `PartitionSpec`s and `NamedSharding`s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l9mbfCXLrYu"
      },
      "source": [
        "## 1. What has changed in JAX?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRyBxdUsLrYv"
      },
      "source": [
        "### State of JAX before: GSPMD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhCIp7cXLrYw"
      },
      "source": [
        "Prior to Shardy, JAX users who partitioned their models across models across multiple devices used [GSPMD](http://go/arxiv/2105.04663) behind the scenes.\n",
        "\n",
        "GSPMD is the propagation+partitioning system that lives in the middle of the XLA pipeline. It operates on HLO - the IR that comes after StableHLO (the program you get after running `jax.jit.lower`).\n",
        "\n",
        "JAX doesn't run GSPMD directly, but encodes instructions into the StableHLO IR for GSPMD to read later on.\n",
        "\n",
        "But before we go any further, let's introduce our working example.\n",
        "\n",
        "Make sure you have installed `jax>=0.4.35`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anTB6ODVHsGg",
        "outputId": "a21fbd07-f350-4a77-bf8a-0512209968f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jax==0.4.35\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax==0.4.35)\n",
            "  Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting ml-dtypes>=0.4.0 (from jax==0.4.35)\n",
            "  Downloading ml_dtypes-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.35) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.35) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.35) (1.13.1)\n",
            "Downloading jax-0.4.35-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl (87.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-dtypes, jaxlib, jax\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.4.35 jaxlib-0.4.35 ml-dtypes-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install jax==0.4.35"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4flIPFVmAYNw"
      },
      "outputs": [],
      "source": [
        "#@title Imports and utilities\n",
        "\n",
        "import os\n",
        "# make sure our code runs on 8 devices\n",
        "os.environ['XLA_FLAGS'] = '--xla_force_host_platform_device_count=8'\n",
        "\n",
        "import jax\n",
        "import numpy as np\n",
        "from jax import numpy as jnp\n",
        "from jax.sharding import Mesh, NamedSharding, PartitionSpec\n",
        "from jax.experimental.shard_map import shard_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl3cXGo9CRMo"
      },
      "source": [
        "First, let's create our mesh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NTq9HXCE5OQ",
        "outputId": "4ffb4bc7-b1e8-4ea7-d706-2758267625f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('data', 4), ('model', 2)])\n"
          ]
        }
      ],
      "source": [
        "mesh = Mesh(\n",
        "    np.reshape(np.array(jax.devices()), (4, 2)),\n",
        "    ('data', 'model'))\n",
        "\n",
        "print(mesh.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjgahNBpNu19"
      },
      "source": [
        "### In/Out shardings\n",
        "\n",
        "Let's look at what changed the most: how sharding attributes are encoded in the JAX program for the compiler to read.\n",
        "\n",
        "Let's look at it through an example. It's going to be an MLP-like model consisting of no bias tensors, and 2 layers (two matmuls)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A--HECIHAe5E"
      },
      "outputs": [],
      "source": [
        "def predict(x, w1, w2):\n",
        "  x = jnp.tanh(x)\n",
        "  z1 = jnp.einsum('ij,jk->ik', x, w1)\n",
        "  z2 = jnp.einsum('ij,jk->ik', z1, w2)\n",
        "  return jnp.sin(z2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLR4IUmXGb7b"
      },
      "source": [
        "What we will want to do here sharding wise is:\n",
        "\n",
        "1. `data` parallelism on x\n",
        "2. `tensor` parallelism on `w1` and `w2` through the [megatron](http://go/arxiv/1909.08053) sharding strategy.\n",
        "\n",
        "Now let's prepare the model for GSPMD sharding. Note that we will explicitly shard `w1`, but let GSPMD propagation shard `w2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZaM4mVKF1UQ",
        "outputId": "c9f7b042-1cc1-4743-a475-8f325ffd2eac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module @jit_predict attributes {mhlo.num_partitions = 8 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<16x128xf32> {mhlo.sharding = \"{devices=[4,1,2]<=[8] last_tile_dim_replicate}\"}, %arg1: tensor<128x256xf32> {mhlo.sharding = \"{devices=[1,2,4]<=[4,2]T(1,0) last_tile_dim_replicate}\"}, %arg2: tensor<256x10xf32>) -> (tensor<16x10xf32> {jax.result_info = \"\"}) {\n",
            "    %0 = stablehlo.tanh %arg0 : tensor<16x128xf32>\n",
            "    %1 = stablehlo.dot_general %0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<16x128xf32>, tensor<128x256xf32>) -> tensor<16x256xf32>\n",
            "    %2 = stablehlo.dot_general %1, %arg2, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<16x256xf32>, tensor<256x10xf32>) -> tensor<16x10xf32>\n",
            "    %3 = stablehlo.sine %2 : tensor<16x10xf32>\n",
            "    return %3 : tensor<16x10xf32>\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def run_in_out_shardings():\n",
        "  samples = jax.ShapeDtypeStruct((16, 128), jnp.float32, sharding=NamedSharding(mesh, PartitionSpec('data', None)))\n",
        "  samples_sharding = NamedSharding(mesh, PartitionSpec('data', None))\n",
        "  w1 = jax.ShapeDtypeStruct((128, 256), jnp.float32, sharding=NamedSharding(mesh, PartitionSpec(None, 'model')))\n",
        "  w1_sharding = NamedSharding(mesh, PartitionSpec(None, 'model'))\n",
        "  w2 = jax.ShapeDtypeStruct((256, 10), jnp.float32)\n",
        "  w2_sharding = None\n",
        "\n",
        "  print(jax.jit(predict, in_shardings=(samples_sharding, w1_sharding, w2_sharding)).lower(samples, w1, w2).as_text())\n",
        "\n",
        "run_in_out_shardings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5oXRrVoPh8P"
      },
      "source": [
        "GSPMD's sharding annotations look like the following:\n",
        "\n",
        "| JAX sharding    | GSPMD sharding |\n",
        "| -------- | ------- |\n",
        "| `NamedSharding(mesh, PartitionSpec('data', None))`  | `{devices=[4,1,2]<=[8] last_tile_dim_replicate}`    |\n",
        "| `NamedSharding(mesh, PartitionSpec(None, 'model'))` | `{devices=[1,2,4]<=[4,2]T(1,0) last_tile_dim_replicate}`     |\n",
        "| `None`    | nothing    |\n",
        "\n",
        "`None` is no sharding as expected since GSPMD will populate this during sharding propagation.\n",
        "\n",
        "Notice how all the axis names go away? While there is a 1:1 correspondance between `NamedSharding` and GSPMD sharding, as a reader, it can be difficult to read. It is only more difficult once you introduce various axis names.\n",
        "\n",
        "Let's look at Shardy for comparison. To enable Shardy in JAX, simply enable the flag:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k7sNUIOQsxm",
        "outputId": "21e13c60-416e-43e5-bdbd-2942b8b3b0a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module @jit_predict attributes {mhlo.num_partitions = 8 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  sdy.mesh @mesh = <[\"data\"=4, \"model\"=2]>\n",
            "  func.func public @main(%arg0: tensor<16x128xf32> {sdy.sharding = #sdy.sharding<@mesh, [{\"data\"}, {}]>}, %arg1: tensor<128x256xf32> {sdy.sharding = #sdy.sharding<@mesh, [{}, {\"model\"}]>}, %arg2: tensor<256x10xf32>) -> (tensor<16x10xf32> {jax.result_info = \"\"}) {\n",
            "    %0 = stablehlo.tanh %arg0 : tensor<16x128xf32>\n",
            "    %1 = stablehlo.dot_general %0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<16x128xf32>, tensor<128x256xf32>) -> tensor<16x256xf32>\n",
            "    %2 = stablehlo.dot_general %1, %arg2, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<16x256xf32>, tensor<256x10xf32>) -> tensor<16x10xf32>\n",
            "    %3 = stablehlo.sine %2 : tensor<16x10xf32>\n",
            "    return %3 : tensor<16x10xf32>\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "jax.config.update(\"jax_use_shardy_partitioner\", True)\n",
        "run_in_out_shardings()\n",
        "jax.config.update(\"jax_use_shardy_partitioner\", False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAhl-fiERfOQ"
      },
      "source": [
        "Now we have\n",
        "\n",
        "| JAX sharding    | Shardy sharding |\n",
        "| -------- | ------- |\n",
        "| `NamedSharding(mesh, PartitionSpec('data', None))`  | `#sdy.sharding<@mesh, [{\"data\"}, {}]>`    |\n",
        "| `NamedSharding(mesh, PartitionSpec(None, 'model'))` | `#sdy.sharding<@mesh, [{}, {\"model\"}]>`     |\n",
        "| `None`    | nothing    |\n",
        "\n",
        "Shardy's representation is a lot closer to what JAX `NamedSharding`s are like. So when looking at a file dump of your program after propagation, it will be a lot easier to understand what is going on since the correspondance is a lot closer to JAX.\n",
        "\n",
        "Note that instead of the total devices/axes living on the sharding, they live on a top level `@mesh` value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg8XG7y9SEVJ"
      },
      "source": [
        "### `jax.lax.with_sharding_constraint`\n",
        "\n",
        "GSPMD currently lowers it to a custom call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O_PB0CDa-OV",
        "outputId": "733ba26e-0bcd-4454-a8aa-381433b7d66a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module @jit_f attributes {mhlo.num_partitions = 8 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<32x64xf32>) -> (tensor<32x64xf32> {jax.result_info = \"\"}) {\n",
            "    %0 = stablehlo.custom_call @Sharding(%arg0) {backend_config = \"unspecified_dims=[1]\", mhlo.sharding = \"{devices=[4,1,2]<=[8] last_tile_dim_replicate}\"} : (tensor<32x64xf32>) -> tensor<32x64xf32>\n",
            "    return %0 : tensor<32x64xf32>\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def run_with_sharding_constraint():\n",
        "  x = jax.ShapeDtypeStruct((32, 64), jnp.float32)\n",
        "\n",
        "  def f(x):\n",
        "    return jax.lax.with_sharding_constraint(x, NamedSharding(mesh, PartitionSpec('data', PartitionSpec.UNCONSTRAINED)))\n",
        "\n",
        "  print(jax.jit(f).lower(x).as_text())\n",
        "\n",
        "run_with_sharding_constraint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5pLLzvsbf6O"
      },
      "source": [
        "But under Shardy it's an explicit op:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haybWp_0bxI8",
        "outputId": "4a450ea7-5f35-4c3d-a06a-f254c89ce9b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module @jit_f attributes {mhlo.num_partitions = 8 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  sdy.mesh @mesh = <[\"data\"=4, \"model\"=2]>\n",
            "  func.func public @main(%arg0: tensor<32x64xf32>) -> (tensor<32x64xf32> {jax.result_info = \"\"}) {\n",
            "    %0 = sdy.sharding_constraint %arg0 <@mesh, [{\"data\"}, {?}]> : tensor<32x64xf32>\n",
            "    return %0 : tensor<32x64xf32>\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "jax.config.update(\"jax_use_shardy_partitioner\", True)\n",
        "run_with_sharding_constraint()\n",
        "jax.config.update(\"jax_use_shardy_partitioner\", False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXxwCyk6KbT-"
      },
      "source": [
        "Note that `UNCONSTRAINED` under GSPMD has the custom call have an op attribute `backend_config = \"unspecified_dims=[1]\"`. But under Shardy, it makes dim 1 be `{?}`. In Shardy, dimension shardings without a `?` are closed, meaning that dimension can't be further sharded, but when it has a trailing `?`, it can be further sharded. Refer to [Sharding representation](https://github.com/openxla/shardy/tree/main/docs/sharding_representation.md) for more info on the sharding representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpHQaPcob5i5"
      },
      "source": [
        "### `jax.experimental.shard_map`\n",
        "\n",
        "Under GSPMD this is a few different custom calls with various `shard_map` specific attributes on the GSPMD sharding. Let's look where the `model` axis is `auto`, meaning it's free to be used inside the body of the shard_map by sharding constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWWjHMLmcNcW",
        "outputId": "00cf1e63-80ed-4c7c-fa44-844ac8739ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module @jit_body attributes {mhlo.num_partitions = 8 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  func.func public @main(%arg0: tensor<32x64xf32>) -> (tensor<32x64xf32> {jax.result_info = \"\"}) {\n",
            "    %0 = stablehlo.custom_call @Sharding(%arg0) {backend_config = \"\", mhlo.sharding = \"{devices=[4,1,2]<=[8] last_tile_dim_replicate}\"} : (tensor<32x64xf32>) -> tensor<32x64xf32>\n",
            "    %1 = stablehlo.custom_call @SPMDFullToShardShape(%0) {backend_config = \"\", mhlo.sharding = \"{manual}\"} : (tensor<32x64xf32>) -> tensor<8x64xf32>\n",
            "    %2 = call @shmap_body(%1) : (tensor<8x64xf32>) -> tensor<32x64xf32>\n",
            "    %3 = stablehlo.custom_call @Sharding(%2) {backend_config = \"\", mhlo.sharding = \"{manual}\"} : (tensor<32x64xf32>) -> tensor<32x64xf32>\n",
            "    %4 = stablehlo.custom_call @SPMDShardToFullShape(%3) {backend_config = \"\", mhlo.sharding = \"{replicated}\"} : (tensor<32x64xf32>) -> tensor<32x64xf32>\n",
            "    return %4 : tensor<32x64xf32>\n",
            "  }\n",
            "  func.func private @shmap_body(%arg0: tensor<8x64xf32>) -> (tensor<32x64xf32> {jax.result_info = \"[None, None]\"}) {\n",
            "    %0 = \"stablehlo.all_gather\"(%arg0) <{all_gather_dim = 0 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 2, 4, 6], [1, 3, 5, 7]]> : tensor<2x4xi64>, use_global_device_ids}> : (tensor<8x64xf32>) -> tensor<32x64xf32>\n",
            "    return %0 : tensor<32x64xf32>\n",
            "  }\n",
            "}\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "def run_shard_map():\n",
        "  x = jax.ShapeDtypeStruct((32, 64), jnp.float32)\n",
        "\n",
        "  def body(x):\n",
        "    return jax.lax.all_gather(x, 'data', tiled=True)\n",
        "\n",
        "  shmaped_f = shard_map(\n",
        "        body,\n",
        "        mesh=mesh,\n",
        "        in_specs=(jax.sharding.PartitionSpec('data',),),\n",
        "        out_specs=jax.sharding.PartitionSpec(),\n",
        "        check_rep=False)\n",
        "\n",
        "  print(jax.jit(shmaped_f).lower(x).as_text())\n",
        "\n",
        "print(run_shard_map())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yug6aSqXc_Sv"
      },
      "source": [
        "With the custom calls and GSPMD sharding, it's getting pretty confusing. Let's look at what Shardy gives:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hCGbWnbdHa-",
        "outputId": "bd6351f9-f430-48f1-c37f-30da033925ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "module @jit_body attributes {mhlo.num_partitions = 8 : i32, mhlo.num_replicas = 1 : i32} {\n",
            "  sdy.mesh @mesh = <[\"data\"=4, \"model\"=2]>\n",
            "  func.func public @main(%arg0: tensor<32x64xf32>) -> (tensor<32x64xf32> {jax.result_info = \"\"}) {\n",
            "    %0 = sdy.manual_computation(%arg0) in_shardings=[<@mesh, [{\"data\"}, {}]>] out_shardings=[<@mesh, [{}, {}]>] manual_axes={\"data\", \"model\"} (%arg1: tensor<8x64xf32>) {\n",
            "      %1 = \"stablehlo.all_gather\"(%arg1) <{all_gather_dim = 0 : i64, channel_handle = #stablehlo.channel_handle<handle = 1, type = 1>, replica_groups = dense<[[0, 2, 4, 6], [1, 3, 5, 7]]> : tensor<2x4xi64>, use_global_device_ids}> : (tensor<8x64xf32>) -> tensor<32x64xf32>\n",
            "      sdy.return %1 : tensor<32x64xf32>\n",
            "    } : (tensor<32x64xf32>) -> tensor<32x64xf32>\n",
            "    return %0 : tensor<32x64xf32>\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "jax.config.update(\"jax_use_shardy_partitioner\", True)\n",
        "run_shard_map()\n",
        "jax.config.update(\"jax_use_shardy_partitioner\", False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f55laSGGdMP2"
      },
      "source": [
        "We now:\n",
        "\n",
        "- Have a single op called `sdy.manual_computation` which holds:\n",
        "  - the `in_specs`\n",
        "  - the `out_specs`\n",
        "  - the body of the shard_map\n",
        "  - the inverse of the `auto` axes which we call `manual_axes`\n",
        "\n",
        "A lot easier to read!\n",
        "\n",
        "Note that `manual_axes` is always equal to the axes in the `mesh` but not in the `auto = frozenset([...])`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UfLgKHie0ai"
      },
      "source": [
        "### Auto partitioners\n",
        "\n",
        "In progress."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTR9tmEJnM6v"
      },
      "source": [
        "### XLA_DUMP_TO\n",
        "\n",
        "When specifying the `XLA_DUMP_TO`, you will see an additional `shardy/` directory containing various dumps of the StableHLO program. A lot of them are currently only relevant to the Shardy team to debug issues. The one you should focus on when debugging is `sdy_module_after_sdy_export.mlir` which is the module after propagation finished on the StableHLO program.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kIyBosUoT_j"
      },
      "source": [
        "## 2. Why Shardy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EdCWagVNiuF"
      },
      "source": [
        "### Readability\n",
        "\n",
        "As seen above, it's much easier to read the shardings and shard_maps and understand how they match what is happening in the JAX code. Similarly GSPMD propagation will give back HLO code - not MLIR which both Shardy and `jax.jit.lower` return."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si1rYISKoYNO"
      },
      "source": [
        "### Interpretability\n",
        "\n",
        "We are planning on exposing a feature we call \"user priorities\" (not in JAX yet!). It allows you to attach a value telling Shardy how important a tensor's dimension sharding is over other constraints in the program.\n",
        "\n",
        "Higher prioritied are defines as lower values (lowest being 0, think of it as a p0 priority tasks).\n",
        "\n",
        "```python\n",
        "PartitionSpec(None, 'x', 'y', priorities=(None, 0, 1))\n",
        "```\n",
        "\n",
        "Here the sharding of dim 1 on `x` has a higher priority than dim 2 on `y`, meaning dim 1 will be propagated through the program first and then dim 2, meaning any potential sharding conflicts will be explicitly avoided by having `x` propagated first.\n",
        "\n",
        "This can be helpful for debugging models as well by having you break down your sharding strategies to separate rounds of propagation in Shardy. For example:\n",
        "\n",
        "* Priority 0: data parallelism\n",
        "* Priority 1: megatron\n",
        "* Priority 2: ZeRO sharding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR9YOSVeUSyy"
      },
      "source": [
        "## FAQS\n",
        "\n",
        "Below is a list of questions you may have on various JAX features and capabilities.\n",
        "\n",
        "### JAX Sharding types\n",
        "\n",
        "#### What about GSPMDSharding?\n",
        "\n",
        "`GSPMDSharding`  is closely tied to the C++/protobuf representation inside the XLA compiler. As such the type itself won't be supported.\n",
        "\n",
        "#### What about PositionalSharding?\n",
        "\n",
        "This won't be supported. Instead use a `NamedSharding` with `device_ids`.\n",
        "\n",
        "#### PmapSharding\n",
        "\n",
        "This won't be supported. Shardy is meant for `jax.jit`, not `jax.pmap`.\n",
        "\n",
        "### Propagation Questions\n",
        "\n",
        "Section for questions about what you may see during propagation.\n",
        "\n",
        "#### What are split Axes in Shardy, aka \"x\":(2)2?\n",
        "\n",
        "Refer to \"Axis splitting and sub-axes\" in [Axis splitting and sub-axes](https://github.com/openxla/shardy/tree/main/docs/sharding_representation.md#axis-splitting-and-sub-axes)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}